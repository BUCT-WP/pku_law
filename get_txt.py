import asyncio
from playwright.async_api import async_playwright
import json
from bs4 import BeautifulSoup
import re
import random
import os

def clean_text_content(text):
    """
    æ¸…ç†æ–‡æœ¬å†…å®¹ï¼Œæ”¹å–„æ¢è¡Œé€»è¾‘å¹¶å»é™¤ä¸éœ€è¦çš„å†…å®¹
    """
    if not text:
        return ""
    
    # å»é™¤"æ³•å®æ–°AI"ç›¸å…³å†…å®¹
    text = re.sub(r'æ³•å®æ–°AI\s*', '', text)
    
    # å¤„ç†æ¢è¡Œé€»è¾‘
    lines = text.split('\n')
    cleaned_lines = []
    
    for i, line in enumerate(lines):
        line = line.strip()
        if not line:  # è·³è¿‡ç©ºè¡Œ
            continue
            
        # å¦‚æœå½“å‰è¡Œå¾ˆçŸ­ï¼ˆå¯èƒ½æ˜¯è¢«é”™è¯¯åˆ†å‰²çš„ï¼‰ï¼Œä¸”ä¸‹ä¸€è¡Œå­˜åœ¨ï¼Œå°è¯•åˆå¹¶
        if (len(line) < 20 and i + 1 < len(lines) and 
            lines[i + 1].strip() and 
            not line.endswith(('ã€‚', 'ï¼›', 'ï¼š', 'ã€', 'ï¼', 'ï¼Ÿ', '"', '"', 'ã€‹', 'ï¼‰'))):
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥åˆå¹¶åˆ°ä¸Šä¸€è¡Œ
            if cleaned_lines and not cleaned_lines[-1].endswith(('ã€‚', 'ï¼›', 'ï¼š', 'ï¼', 'ï¼Ÿ', '"', '"', 'ã€‹')):
                cleaned_lines[-1] += line
            else:
                cleaned_lines.append(line)
        else:
            # å¦‚æœä¸Šä¸€è¡Œå¾ˆçŸ­ä¸”å½“å‰è¡Œä¸æ˜¯æ ‡é¢˜æˆ–ç¼–å·ï¼Œå°è¯•åˆå¹¶åˆ°ä¸Šä¸€è¡Œ
            if (cleaned_lines and len(cleaned_lines[-1]) < 20 and 
                not re.match(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ã€ï¼]', line) and
                not re.match(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+æ¡', line) and
                not re.match(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« ', line)):
                cleaned_lines[-1] += line
            else:
                cleaned_lines.append(line)
    
    # åˆå¹¶åå†æ¬¡å¤„ç†ï¼Œç¡®ä¿æ®µè½å®Œæ•´æ€§
    final_lines = []
    i = 0
    while i < len(cleaned_lines):
        current_line = cleaned_lines[i]
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸ä¸‹ä¸€è¡Œåˆå¹¶
        while (i + 1 < len(cleaned_lines) and 
               not current_line.endswith(('ã€‚', 'ï¼›', 'ï¼', 'ï¼Ÿ', '"', '"', 'ã€‹')) and
               not re.match(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ã€ï¼]', cleaned_lines[i + 1]) and
               not re.match(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[æ¡ç« ]', cleaned_lines[i + 1])):
            i += 1
            if i < len(cleaned_lines):
                current_line += cleaned_lines[i]
        
        final_lines.append(current_line)
        i += 1
    
    # é‡æ–°ç»„ç»‡æ–‡æœ¬ï¼Œç¡®ä¿é€‚å½“çš„æ®µè½åˆ†éš”
    result = []
    for line in final_lines:
        if line.strip():
            result.append(line.strip())
    
    return '\n'.join(result)

async def get_page_response(url):
    """
    ä½¿ç”¨Playwrightè·å–æŒ‡å®šURLçš„å“åº”
    """
    async with async_playwright() as p:
        # å¯åŠ¨æµè§ˆå™¨
        browser = await p.chromium.launch(headless=True)
        
        # åˆ›å»ºæ–°é¡µé¢
        page = await browser.new_page()
        
        # è®¾ç½®å®Œæ•´çš„è¯·æ±‚å¤´ï¼Œæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
        await page.set_extra_http_headers({
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br, zstd',
            'Accept-Language': 'zh-CN,zh;q=0.9',
            'Cache-Control': 'max-age=0',
            'Cookie': 'referer=https://www.pkulaw.com/chl/2a002e408a3b3827bdfb.html; CookieId=0cce6def6e9471542b9375e01f6ae1f9; SUB=a4e77e87-4f16-4911-b968-47c4b8a36245; preferred_username=%E5%8C%97%E4%BA%AC%E5%8C%96%E5%B7%A5%E5%A4%A7%E5%AD%A6; loginType=ip; session_state=0f449870-da38-4bb2-b489-9950c6225e80; xCloseNew=2; _bl_uid=Fzmjtct6jCet7dxXw1n8eastsO0F; pkulaw_v6_sessionid=aay1kpuydp5emqoow2z5l4cw; Hm_lvt_8266968662c086f34b2a3e2ae9014bf8=1751332003,1751339146,1751369195; HMACCOUNT=C9000E37F559039A; cookieUUID=cookieUUID_1751369195747; userislogincookie=always; LoginAccount=%e5%8c%97%e4%ba%ac%e5%8c%96%e5%b7%a5%e5%a4%a7%e5%ad%a6; UserAuthAssetAid=; authormes=b31681372881bca73e55f754376488ab3655642886719161605a2f1b2075ef9335b7d5e9345c01a9bdfb; Hm_lpvt_8266968662c086f34b2a3e2ae9014bf8=1751369198; Hm_up_8266968662c086f34b2a3e2ae9014bf8=%7B%22ysx_yhqx_20220602%22%3A%7B%22value%22%3A%220%22%2C%22scope%22%3A1%7D%2C%22ysx_hy_20220527%22%3A%7B%22value%22%3A%2201%22%2C%22scope%22%3A1%7D%2C%22uid_%22%3A%7B%22value%22%3A%225f634e21-d5b1-ed11-b393-00155d3c0709%22%2C%22scope%22%3A1%7D%2C%22ysx_yhjs_20220602%22%3A%7B%22value%22%3A%222%22%2C%22scope%22%3A1%7D%7D',
            'Referer': 'https://www.pkulaw.com/chl/2a002e408a3b3827bdfb.html',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'same-origin',
            'Sec-Fetch-User': '?1',
            'Upgrade-Insecure-Requests': '1',
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36',
            'sec-ch-ua': '"Google Chrome";v="137", "Chromium";v="137", "Not/A)Brand";v="24"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"macOS"'
        })
        
        try:
            # è®¿é—®é¡µé¢
            response = await page.goto(url, wait_until='networkidle')
            
            # è·å–å“åº”ä¿¡æ¯
            response_data = {
                'url': response.url,
                'status': response.status,
                'status_text': response.status_text,
                'headers': dict(response.headers),
                'content': await page.content(),
                'title': await page.title()
            }
            
            # print(f"çŠ¶æ€ç : {response.status}")
            # print(f"é¡µé¢æ ‡é¢˜: {await page.title()}")
            # print(f"å“åº”URL: {response.url}")
            # print(f"å“åº”å¤´: {json.dumps(dict(response.headers), indent=2, ensure_ascii=False)}")
            # print(f"é¡µé¢å†…å®¹é•¿åº¦: {len(await page.content())} å­—ç¬¦")
            
            # è·å–å®Œæ•´é¡µé¢å†…å®¹
            page_content = await page.content()
            
            # ä½¿ç”¨BeautifulSoupè§£æHTML
            soup = BeautifulSoup(page_content, 'html.parser')
            
            # åˆ›å»ºlawæ–‡ä»¶å¤¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            law_folder = "law"
            if not os.path.exists(law_folder):
                os.makedirs(law_folder)
                print(f"åˆ›å»ºæ–‡ä»¶å¤¹: {law_folder}")
            
            # æŸ¥æ‰¾class=titleçš„å…ƒç´ ä½œä¸ºæ–‡ä»¶å
            title_element = soup.find('h2', class_='title')
            if title_element:
                title_text = title_element.get_text(strip=True)
                # æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤ä¸å…è®¸çš„å­—ç¬¦
                import re
                safe_filename = re.sub(r'[<>:"/\\|?*]', '_', title_text)
                safe_filename = safe_filename.strip()[:100]  # é™åˆ¶é•¿åº¦
                filename = os.path.join(law_folder, f"{safe_filename}.txt")
            else:
                filename = os.path.join(law_folder, "divFullText_content.txt")
                print("æœªæ‰¾åˆ°class='title'çš„å…ƒç´ ï¼Œä½¿ç”¨é»˜è®¤æ–‡ä»¶å")
            
            # æŸ¥æ‰¾idä¸ºdivFullTextçš„divå…ƒç´ 
            div_full_text = soup.find('div', id='divFullText')
            
            if div_full_text:
                # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ ï¼ˆå¦‚"æ³•å®æ–°AI"ç›¸å…³çš„å…ƒç´ ï¼‰
                # æŸ¥æ‰¾å¹¶ç§»é™¤åŒ…å«"æ³•å®æ–°AI"çš„å…ƒç´ 
                for element in div_full_text.find_all(text=lambda text: text and "æ³•å®æ–°AI" in text):
                    element.extract()
                
                # ç§»é™¤å¯èƒ½åŒ…å«å¹¿å‘Šæˆ–æ— å…³å†…å®¹çš„ç‰¹å®šæ ‡ç­¾
                for tag in div_full_text.find_all(['script', 'style', 'iframe']):
                    tag.decompose()
                
                # ç§»é™¤åŒ…å«"æ³•å®æ–°AI"çš„çˆ¶å…ƒç´ 
                for element in div_full_text.find_all():
                    if element.get_text(strip=True) == "æ³•å®æ–°AI":
                        element.decompose()
                
                # è·å–divå…ƒç´ çš„çº¯æ–‡æœ¬å†…å®¹
                div_text_content = div_full_text.get_text(separator='\n', strip=True)
                
                # æ¸…ç†æ–‡æœ¬å†…å®¹
                cleaned_content = clean_text_content(div_text_content)
                
                # ä¿å­˜æ¸…ç†åçš„æ–‡æœ¬å†…å®¹åˆ°lawæ–‡ä»¶å¤¹
                with open(filename, 'w', encoding='utf-8') as f:
                    f.write(cleaned_content)
                print(f"divFullTextçº¯æ–‡æœ¬å†…å®¹å·²ä¿å­˜åˆ° {filename}")
                
                # æ‰“å°ä¸€äº›åŸºæœ¬ä¿¡æ¯
                print(f"divFullTextæ–‡æœ¬é•¿åº¦: {len(cleaned_content)} å­—ç¬¦")
                
                # æ‰“å°å†…å®¹é¢„è§ˆ
                preview = cleaned_content[:200] + "..." if len(cleaned_content) > 200 else cleaned_content
                print(f"å†…å®¹é¢„è§ˆ: {preview}")
                
            else:
                print("æœªæ‰¾åˆ°idä¸ºdivFullTextçš„å…ƒç´ ")
                # æ‰“å°é¡µé¢ä¸­æ‰€æœ‰divå…ƒç´ çš„idï¼Œå¸®åŠ©è°ƒè¯•
                all_divs = soup.find_all('div', id=True)
                print("é¡µé¢ä¸­å‘ç°çš„divå…ƒç´ id:")
                for div in all_divs[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                    print(f"  - {div.get('id')}")
            
            return response_data
            
        except Exception as e:
            print(f"è·å–é¡µé¢æ—¶å‡ºé”™: {e}")
            return None
        finally:
            # å…³é—­æµè§ˆå™¨
            await browser.close()

async def process_urls_from_file(file_path):
    """
    ä»æ–‡ä»¶ä¸­è¯»å–URLå¹¶é€ä¸ªå¤„ç†ï¼Œå¤„ç†å®Œä¸€ä¸ªåˆ é™¤ä¸€ä¸ª
    """
    urls_file = os.path.abspath(file_path)
    
    if not os.path.exists(urls_file):
        print(f"æ–‡ä»¶ {urls_file} ä¸å­˜åœ¨")
        return
    
    total_processed = 0
    
    while True:
        # è¯»å–å½“å‰æ–‡ä»¶ä¸­çš„æ‰€æœ‰URL
        try:
            with open(urls_file, 'r', encoding='utf-8') as f:
                urls = [line.strip() for line in f.readlines() if line.strip()]
        except Exception as e:
            print(f"è¯»å–URLsæ–‡ä»¶æ—¶å‡ºé”™: {e}")
            break
        
        # å¦‚æœæ²¡æœ‰URLäº†ï¼Œç»“æŸå¤„ç†
        if not urls:
            print("æ‰€æœ‰URLå·²å¤„ç†å®Œæˆï¼")
            break
        
        # å–ç¬¬ä¸€ä¸ªURLè¿›è¡Œå¤„ç†
        current_url = urls[0]
        remaining_urls = urls[1:]
        
        print(f"\n{'='*60}")
        print(f"æ­£åœ¨å¤„ç†ç¬¬ {total_processed + 1} ä¸ªURL")
        print(f"å‰©ä½™URLæ•°é‡: {len(remaining_urls)}")
        print(f"å½“å‰URL: {current_url}")
        print(f"{'='*60}")
        
        # å¤„ç†å½“å‰URL
        response_data = await get_page_response(current_url)
        
        if response_data:
            print(f"âœ… URLå¤„ç†æˆåŠŸ: {current_url}")
        else:
            print(f"âŒ URLå¤„ç†å¤±è´¥: {current_url}")
        
        # æ›´æ–°æ–‡ä»¶ï¼Œç§»é™¤å·²å¤„ç†çš„URL
        try:
            with open(urls_file, 'w', encoding='utf-8') as f:
                for url in remaining_urls:
                    f.write(url + '\n')
            print(f"å·²ä»æ–‡ä»¶ä¸­ç§»é™¤å¤„ç†å®Œçš„URL")
        except Exception as e:
            print(f"æ›´æ–°URLsæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        
        total_processed += 1
        
        # å¦‚æœè¿˜æœ‰å‰©ä½™URLï¼Œç­‰å¾…éšæœºé—´éš”
        if remaining_urls:
            wait_time = random.uniform(1, 5)
            print(f"ç­‰å¾… {wait_time:.1f} ç§’åå¤„ç†ä¸‹ä¸€ä¸ªURL...")
            await asyncio.sleep(wait_time)
    
    print(f"\nğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆï¼æ€»å…±å¤„ç†äº† {total_processed} ä¸ªURL")

async def main():
    urls_file = "urls.txt"
    print(f"å¼€å§‹æ‰¹é‡å¤„ç†URLæ–‡ä»¶: {urls_file}")
    
    await process_urls_from_file(urls_file)

if __name__ == "__main__":
    asyncio.run(main())